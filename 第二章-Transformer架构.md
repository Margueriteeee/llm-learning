# 第二章 Transformer架构

## 2.1 注意力机制
NLP 逐渐从最初的统计学习模型进行文本表示的向量空间模型、语言模型，通过Word2Vect的单层神经网络进入到通过神经网络学习文本表示的时代。

从计算机视觉为起源的神经网络的核心架构有三种：

1. 全连接神经网络（Feedforward Neural Network，FNN）：每一层的神经元都和上下两层的每一个神经元完全链接  
![Alt text](image.png)  
2. 卷积神经网络（Convolutional Neural Network，CNN）：训练参数量远小于全连接神经网络的卷积层来进行特征提取和学习
![Alt text](image-1.png)  
3. 循环神经网络（Recurrent Neural Network，RNN），能够使用历史信息作为输入、包含环和自重复的网络  
![Alt text](image-2.png)  

注意！NLP的输入文本通常是序列格式，而RNN就是专用于 **处理序列、时序数据** 的神经网络，类似地LSTM也非常适合于捕捉时序信息并生成序列  

但 RNN 和 LSTM 有两个难以弥补的缺陷：  
1. 序列计算模式限制了并行计算的能力，而GPU只有在并行计算上能发挥出最大的优势  
2. RNN 难以捕捉长序列的相关关系。距离越远的输入关系越难被捕捉，在读取时需要把整个序列都读入内存然后以此计算，内存大小也限制了每次读取的长度  

所以，Transformer诞生了：完全由注意力机制构成的神经网络 
> " *LLM 的鼻祖及核心架构* "

### 2.1.1 什么是注意力机制
注意力机制由三个核心变量构成：  
- 查询值 Query
- 键值 Key
- 真值 Value

计算过程：  
- 计算 Query 和 Key 的相关性得到一个权重
  - 反映了从 查询内容出发，对文本每一个token应该分配的注意力相对大小  
- 把计算得到的权重和Value进行运算，得到最终的注意力结果

注意力机制的本质仍然是 ***加权求和*** ，通过计算查询和键值的相关性为真值进行加权，从而获得真值中每个token应该被分配的注意力 —— *拟合序列中每个词同其他词的相关关系*

### 2.1.2 深入理解注意力机制
在文本表示中，词向量被用来表征语义信息，语言模型在训练过程中的目标也是让语义相近的词在 **向量空间中的距离** 更近，语义较远的词在向量空间中的距离更远

通常使用 **欧氏距离** 来衡量词向量的相似性，也可以使用 ***点积*** 来衡量：
- 通过计算点积来计算词之间的相似度
- 再借助 softmax 函数把相似度转换到（0，1）之间的权重
- 这样得到的向量反映了query和每一个key的相似程度，同时权重相加为1，也就是注意力分数
- 再把权重和值向量对应相乘，即为注意力机制的基本公式
- ![Alt text](image-3.png)  
- 进一步把值变成多维度的向量，就能同时一次性查询多个Query：
- ![Alt text](image-4.png)
- 如果Q, K对应的维度比较大，softmax缩放的时候很容易导致不同值之间的差异很大，从而会影响梯度的稳定性。因此需要把Q,K相乘得到的权重做一个缩放：
- ![Alt text](image-5.png)

### 2.1.3 注意力实现

~~~python
{
    def attention(query, key, value, dropout=None):
        d_k = query.size(-1)  # 获取键向量的维度（键和值的维度相同）
        scores = torch.matmul(query, key.transpose(-2, -1)) / math.sqrt(d_k)
        # sofrmax
        p_attn = scores.softmax(dim=-1)
        if dropout is not None:
            p_attn = dropout(p_attn)
        # 根据注意力分数加权求和
        return torch.matmul(p_attn, value), p_attn 
}
~~~

### 2.1.4 自注意力
注意力机制的本质：
- 对两段序列的元素依次进行相似度计算，找出每一个元素对另一序列中每个元素的相关度
- 基于相关度对真值进行加权，即分配注意力

但实际应用中，往往只需要Q K之间的注意力结果，很少存在额外的真值Value
- 所以只需要拟合两个文本序列得到二者的注意力分数即可
- Q 往往来自于一个序列，K 与 V 来自于另一个序列，都通过参数矩阵计算得到，从而可以拟合这两个序列之间的关系

自注意力机制就是计算本身序列中每一个元素对其它元素的注意力分布，也就是QKV都为本身这一个序列
- 具体实现中，Q、K、V由 **同一个输入**，经过 **不同的参数矩阵** 计算得到

自注意力的代码实现就是把qkv都传入同一个参数  
~~~python
    attention(x, x, x)
~~~

### 2.1.5 掩码自注意力
掩码的作用：遮蔽一些特定位置的token，让模型在学习的过程中将 masked token 忽略掉

动机：让模型只能使用历史信息进行预测，而不能看到当前元素后面的元素（未来信息）
- 对一个文本序列，不断根据之前的token来预测下一个token，直到补全整个文本序列

但上述是一个 **串行过程** ，Transformer针对串行计算效率低的问题，提出了掩码自注意力的方法：  
- 生成一串掩码，遮蔽当前元素后面的所有token
- 一个序列masked之后被并行输入模型中
- 模型需要根据拿到的序列中可以看到的token预测 masked 的token：
- ![Alt text](image-6.png)

实现：  
- 掩码实际上是一个和文本序列等长的**上三角矩阵**
- 创建一个这样的上三角矩阵作为掩码，把对应位置的输入遮蔽掉（赋值为无限小）即可

~~~python
# 创建一个上三角矩阵，用于遮蔽未来信息。
# 先通过 full 函数创建一个 1 * seq_len * seq_len 的矩阵
mask = torch.full((1, args.max_seq_len, args.max_seq_len), float("-inf"))
# triu 函数的功能是创建一个上三角矩阵
mask = torch.triu(mask, diagonal=1)
~~~

该 Mask 矩阵上三角的元素均为 -inf，其他位置赋值0

在注意力计算时，把得到的注意力分数和 Mask 作和，再进行softmax操作

求和后，注意力分数矩阵的上三角区域结果都变成 -inf ，下三角区域不变。再经过softmax，-inf 的位置会被置为0，从而忽略了上三角区域的注意力分数实现遮蔽

### 2.1.6 多头注意力
