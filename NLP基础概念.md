# 第一章 NLP基础概念

## 1.1 什么是NLP
NLP: Natural Language Processing 自然语言处理  
是一种让计算机理解、解释和生成人类语言的技术  

核心任务：让计算机通过程序模拟人类对语言的认知和应用过程  

常见的语言处理任务：中文分词、子词切分、词性标注、文本分类、实体识别、关系抽取、文本摘要、机器翻译、自动问答......

面临的挑战：处理语言的歧义、理解抽象概念、处理隐喻和讽刺  
手段：  
- 更先进的算法、更大规模的数据集、更精细的语言模型  

## 1.2 NLP发展历程

**从早期的规则基础方法，到符号主义与统计方法，再到现在的机器学习和深度学习方法**

图灵测试（1950）：是判断机器是否能够展现出与人类不可区分的智能行为的测试  
> 如果一台机器可以通过使用打字机成为对话的一部分，并且能够完全模仿人类，没有明显的差异，那么机器被认为是可以思考的  

1980s，随着 **计算能力的提升** 和 **机器学习算法** 的引入，NLP领域的统计模型开始取代复杂的手写规则  

2000s，深度学习技术使得NLP取得了显著的进步  
循环神经网络（Recurrent Neural Network，RNN）、长短时记忆网络（Long Short-Term Memory，LSTM）和注意力机制等   

- 2013：Word2Vec模型，词向量表示  
- 2018：BERT模型，预训练语言模型
- 2023：基于Transformer的模型，如GPT-3，巨大参数的模型

## 1.3 NLP任务

### 1.3.1 中文分词
中文文本没有英文明显的空格分开，需要将连续的文本切分成有意义的词汇序列  

分词是中文文本处理的*首要步骤* -> embedding

**分词不准确将直接影响到整个文本处理后续流程的效果**  

### 1.3.2 子词切分
将词汇进一步分解为更小的单元

特别适用于处理词汇稀疏问题，遇到罕见词的时候，可以通过已知的子词单位理解或生成这些新词  

基本思想：将单词分解成更小的、频繁出现的片段，这些片段可以是单个字符、字符组合或者词根和词缀

### 1.3.3 词性标注
目标：为每个单词分配一个词性标签。基于预先定义的词性标签集

作用：帮助模型理解句子结构、分析与法、进行语义角色标注等等，从而完成信息提取、情感分析、机器翻译等更复杂的处理

方法：通常依赖于机器学习模型，隐马尔可夫模型（Hidden Markov Model，HMM）、条件随机场（Conditional Random Field，CRF）或者基于深度学习的循环神经网络 RNN 和长短时记忆网络 LSTM 等。（需要**大量的标注数据训练**）

### 1.3.4 文本分类
目标：将给定的文本自动分配到一个或多个预定义的类别中

应用：情感分析、垃圾邮件检测、新闻分类、主题识别

关键：**合适的特征表示和分类算法，以及高质量的训练数据**。（通常使用神经网络捕捉到文本数据中的复杂模式和语义信息）

### 1.3.5 实体识别
Named Entity Recognition, NER  

目标：自动识别文本中具有特定意义的实体，并将他们分类为预定义的类别，人名、地点、时间、组织等等

类似于人的语言认知中的“常识” 或 “固定搭配” 

### 1.3.6 关系抽取
目标：识别实体中的语义关系，因果关系、拥有关系、亲属关系、地理位置关系等等

作用：从文本中提取有用的信息用于后续的下游任务（构建知识图谱、问答系统等）

### 1.3.7 文本摘要
类别：**抽取式摘要** 和 **生成式摘要**

- 抽取式摘要：直接从原文中选取关键句子或短语来组成摘要。
  - 优点：信息完全来自原文，准确性较高。
  - 缺点：仅仅是原文中句子的拼接，结果可能不够流畅。
- 生成式摘要：不仅涉及选择文本片段，还需要对这些片段进行重新组织和改写，并生成新的内容。
  - 难度更大，因为它需要理解文本的深层含义，并能够以新的方式表达相同的信息。
  - 需要更复杂的模型，如基于注意力机制的序列到序列模型（Seq2Seq）

## 1.3.8 机器翻译
核心任务

目标：将一种自然语言自动翻译成另一种自然语言

难点：不仅涉及词汇的直接转换，还需要理解源文本的语义、风格和文化背景等

模型：基于神经网络的Seq2Seq模型、Transformer模型等，学习到复杂的语言映射关系

### 1.3.9 自动问答
Automatic Question Answering, QA：高级任务，理解问题并回答问题

内涵：事实查询、复杂的推理和解释，涉及NLP多个子任务，信息检索、文本理解、知识表示和推理等等

分类：  
- 检索式问答（Retrieval-based QA）：通过搜索引擎等方式从大量文本中检索答案
- 知识库问答（Knowledge-based QA）：结构化的知识库
- 社区问答（Community-based QA）：依赖于用户生成的问答数据，如问答社区、论坛等

> 上述任务其实都基于一点：**文本数据数字化**  
> 也就是把文本转换成计算机能够处理的格式，如向量、矩阵等数据结构

## 1.4 文本表示的发展历程

### 1.4.1 词向量
向量空间模型（Vector Space Model, VSM）是 NLP 领域中一个基础且强大的文本表示方法

原理：将文本转换为**高维**空间中的向量，实现文本的数学化表示  
- 每个维度代表一个**特征项**（字、词、词组、短语）
- 向量中的每个元素值代表**该特征在文本中的权重**
- 权重有特定的计算公式：词频TF、逆文档频率TF-IDF等  

挑战：（高维空间的通病）数据稀疏性和维数灾难  
- 特征项数量庞大导致向量维度极高
- 多数元素的值是0，一个向量中通常只有少数元素有值

改进：文本结构信息，特征项的选择，权重计算方法  
- 特征表示：借助图方法、主题方法抽取关键词  
- 特征项权重计算：融合改进现有方法、提出新方法

### 1.4.2 语言模型
基于统计的语言模型，广泛应用于语音识别、手写识别、拼写纠错、机器翻译和搜索引擎等众多任务

**核心思想**：马尔科夫假设 —— 一个词的出现概率只依赖于其前面的 N-1 个词，本质是条件概率表示（N代表连续出现单词的数量，可以是任意正整数）
- 基于前 N-1 个词出现的概率乘积，得到第 N 个词出现的概率、
- **条件概率链式规则**

局限：  
- N 较大时也容易出现**数据稀疏性问题**，参数空间急剧增大，同一序列出现的概率变得非常低，导致模型无法有效学习，泛化能力下降  
- 同时也忽略了词之间的**范围依赖**关系，无法处理复杂结构和语义信息

### 1.4.3 Word2Vec
词嵌入技术：基于神经网络的语言模型，根据上下文关系生成词的密集向量表示

**核心思想：**利用词在文本中的上下文信息来捕捉词之间的语义关系，从而使得语义相似或相关的词在向量空间中距离较近

主要架构（基于局部上下文）：  
- 连续词袋模型CBOW(Continuous Bag of Words)：根据目标词上下文中的词对应的词向量, 计算并输出目标词的向量表示
- Skip-Gram：利用目标词的向量表示计算上下文中的词向量（与CBOW相反）
- **实践验证CBOW适用于小型数据集, 而Skip-Gram在大型语料中表现更好**

优缺点：  
- 生成的是低维（通常几百维）的密集向量，有助于减少计算复杂度和存储需求
- 基于局部上下文捕捉词语词之间的语义关系，因此也很好泛化到未见过的词
- 但也无法捕捉到长距离的依赖关系，缺乏整体的词语关系认知

***基于上下文信息的特征提取有较好的泛化能力***  
- 因为基于上下文提取特征能够使网络学习到语言模式
- 但基于上下文的劣势就是缺乏整体的依赖关系

### 1.4.4 ELMo
Embeddings from Language Models：实现了一词多义、静态词向量到动态词向量的跨越式转变

过程：  
1. 预训练：在大型语料库上训练语言模型，得到词向量模型  
2. 应用到特定任务上进行模型微调，从预训练网络中提取补充道下游任务中，得到更适合于**对应单词的词向量作为新特征**特定任务的词向量  

*首次引入 **预训练思想** 到词向量的生成中*

优缺点：  
- 能够捕捉到词汇的多义性和上下文信息，生成的词向量更丰富、更准确
- 但基于RNN的LSTM模型训练时间长，模型复杂度高、训练时间长、计算资源消耗大
- 关键在于精确的特征提取

# 思考
- 基于大量、优质的文本数据训练出性能较好的语言模型
- 特征提取更精细、更准确的模型对计算资源的要求高，训练时间长（提高**计算资源的利用效率**、提高计算资源的性能）
- 预训练思想：站在巨人的肩膀上
- 提升泛化能力：增强上下文特征的提取和建模，同时不要放弃整体的依赖关系
- 如何处理高维数据问题，对存储和处理提出了要求 —— 基于局部上下文生成低维的密集信息，但同时损失了一部分整体依赖